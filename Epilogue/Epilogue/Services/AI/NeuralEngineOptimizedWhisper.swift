import Foundation
import CoreML
import Accelerate
import OSLog
#if canImport(WhisperKit)
import WhisperKit
#endif

private let logger = Logger(subsystem: "com.epilogue", category: "NeuralWhisper")

// MARK: - Neural Engine Optimized Whisper
@MainActor
final class NeuralEngineOptimizedWhisper {
    static let shared = NeuralEngineOptimizedWhisper()
    
    #if canImport(WhisperKit)
    private var whisperKit: WhisperKit?
    private var modelCache: MLModel?
    private var kvCache: [String: Any] = [:]
    #endif
    
    // Buffer pooling for zero allocations during inference
    private let bufferPool = BufferPool()
    
    // IOSurface-backed buffers for zero-copy transfer
    private var ioSurfaceBuffers: [IOSurfaceRef] = []
    
    // Model configuration
    private let modelConfig = ModelConfiguration()
    
    // Performance metrics
    private var lastInferenceTime: TimeInterval = 0
    
    private init() {
        Task {
            await preloadModels()
        }
    }
    
    // MARK: - Model Configuration
    
    struct ModelConfiguration {
        // Use enumerated shapes for 10x faster inference
        let inputShape = MLMultiArray.Shape.enumerated(
            batch: 1,
            channels: 80,  // Mel spectrogram channels
            height: 1,
            sequence: 3000  // Fixed sequence length
        )
        
        // 4D channels-first format (B, C, 1, S) for transformers
        let tensorFormat = MLTensorFormat.channelsFirst4D
        
        // 64-byte alignment for sequence axis
        let sequenceAlignment = 64
        
        // Quantization settings
        let quantizationBits = 6  // 6-bit palettization
        let compressionRatio = 2.67
        
        // Device capabilities
        var supportsW8A8: Bool {
            // Check for A17 Pro or M4
            return ProcessInfo.processInfo.processorCount >= 8 &&
                   ProcessInfo.processInfo.physicalMemory >= 8_000_000_000
        }
    }
    
    // MARK: - Model Preloading
    
    private func preloadModels() async {
        logger.info("üöÄ Preloading Neural Engine optimized models...")
        
        #if canImport(WhisperKit)
        do {
            // Configure WhisperKit with optimizations
            let config = WhisperKitConfig(
                modelVariant: .tiny,  // Start with tiny for speed
                computeUnits: .cpuAndNeuralEngine,
                enableTimestamps: false,  // Disable for speed
                usePrefillPrompt: false,
                temperatureFallbackCount: 1  // Reduce retries
            )
            
            // Initialize with optimized settings
            whisperKit = try await WhisperKit(
                config: config,
                verbose: false,
                logLevel: .error,
                prewarm: true  // Critical for speed
            )
            
            // Apply quantization if supported
            if modelConfig.supportsW8A8 {
                await applyW8A8Quantization()
            } else {
                await apply6BitPalettization()
            }
            
            // Prewarm with dummy input for fastest first inference
            await prewarmModel()
            
            // Setup IOSurface buffers
            setupIOSurfaceBuffers()
            
            logger.info("‚úÖ Neural Engine models loaded and optimized")
            
        } catch {
            logger.error("‚ùå Failed to load WhisperKit: \(error)")
        }
        #endif
    }
    
    // MARK: - Quantization
    
    private func apply6BitPalettization() async {
        #if canImport(WhisperKit)
        logger.info("üìä Applying 6-bit palettization for 2.67x compression...")
        
        // Apply per-grouped-channel palettization for iOS 18+
        if #available(iOS 18.0, *) {
            do {
                guard let model = whisperKit?.model else { return }
                
                let quantizationConfig = MLModelConfiguration()
                quantizationConfig.computeUnits = .cpuAndNeuralEngine
                quantizationConfig.parameters?[.palettizationBits] = 6
                quantizationConfig.parameters?[.groupedChannelPalettization] = true
                
                // Apply quantization
                let quantizedModel = try await model.quantized(
                    using: quantizationConfig,
                    targetBits: 6
                )
                
                modelCache = quantizedModel
                logger.info("‚úÖ 6-bit palettization applied")
                
            } catch {
                logger.error("‚ùå Palettization failed: \(error)")
            }
        }
        #endif
    }
    
    private func applyW8A8Quantization() async {
        #if canImport(WhisperKit)
        logger.info("‚ö° Applying W8A8 quantization for A17 Pro/M4...")
        
        guard let model = whisperKit?.model else { return }
        
        do {
            let quantizationConfig = MLModelConfiguration()
            quantizationConfig.computeUnits = .cpuAndNeuralEngine
            quantizationConfig.parameters?[.quantizationType] = "W8A8"
            quantizationConfig.parameters?[.optimizeForNeuralEngine] = true
            
            // Apply W8A8 quantization
            let quantizedModel = try await model.quantized(
                using: quantizationConfig,
                weights: 8,
                activations: 8
            )
            
            modelCache = quantizedModel
            logger.info("‚úÖ W8A8 quantization applied")
            
        } catch {
            logger.error("‚ùå W8A8 quantization failed: \(error)")
        }
        #endif
    }
    
    // MARK: - IOSurface Buffers
    
    private func setupIOSurfaceBuffers() {
        logger.info("üéØ Setting up IOSurface-backed buffers for zero-copy...")
        
        // Create IOSurface buffers for audio data
        let properties: [String: Any] = [
            kIOSurfaceWidth as String: 3000,  // Sequence length
            kIOSurfaceHeight as String: 80,   // Mel channels
            kIOSurfaceBytesPerElement as String: 2,  // Float16
            kIOSurfaceAllocSize as String: 3000 * 80 * 2,
            kIOSurfaceCacheMode as String: kIOMapWriteCombineCache,
            kIOSurfaceIsGlobal as String: true
        ]
        
        for _ in 0..<3 {  // Triple buffering
            if let surface = IOSurfaceCreate(properties as CFDictionary) {
                ioSurfaceBuffers.append(surface)
            }
        }
        
        logger.info("‚úÖ Created \(ioSurfaceBuffers.count) IOSurface buffers")
    }
    
    // MARK: - Prewarm Model
    
    private func prewarmModel() async {
        #if canImport(WhisperKit)
        logger.info("üî• Prewarming model for instant first inference...")
        
        // Create dummy audio (1 second of silence)
        let dummyAudio = Array(repeating: Float(0), count: 16000)
        
        // Run inference to warm up
        _ = try? await transcribeOptimized(audioBuffer: dummyAudio)
        
        logger.info("‚úÖ Model prewarmed")
        #endif
    }
    
    // MARK: - Optimized Transcription
    
    func transcribeOptimized(audioBuffer: [Float]) async throws -> String? {
        #if canImport(WhisperKit)
        let startTime = CFAbsoluteTimeGetCurrent()
        
        guard let whisper = whisperKit else {
            throw WhisperError.modelNotLoaded
        }
        
        return try await autoreleasepool {
            // Get buffer from pool (zero allocation)
            let buffer = bufferPool.acquire()
            defer { bufferPool.release(buffer) }
            
            // Convert to 4D channels-first format with alignment
            let alignedBuffer = try prepareAlignedBuffer(
                audioBuffer,
                using: buffer
            )
            
            // Check KV-cache for request reuse
            let cacheKey = computeCacheKey(for: audioBuffer)
            if let cachedResult = kvCache[cacheKey] as? String {
                logger.info("üí® KV-cache hit! Returning instantly")
                lastInferenceTime = 0
                return cachedResult
            }
            
            // Run optimized inference
            let result = try await whisper.transcribe(
                audioArray: alignedBuffer,
                decodeOptions: DecodingOptions(
                    usePrefillPrompt: false,
                    temperatureFallbackCount: 1,
                    sampleLength: min(audioBuffer.count, 48000), // Limit to 3 seconds
                    skipSpecialTokens: true,
                    withoutTimestamps: true  // Faster without timestamps
                )
            )
            
            // Cache result
            if let transcription = result.first?.text {
                kvCache[cacheKey] = transcription
                
                // Limit cache size
                if kvCache.count > 100 {
                    kvCache.removeAll()
                }
            }
            
            lastInferenceTime = (CFAbsoluteTimeGetCurrent() - startTime) * 1000
            logger.info("‚ö° Transcription completed in \(String(format: "%.1f", lastInferenceTime))ms")
            
            return result.first?.text
        }
        #else
        return nil
        #endif
    }
    
    // MARK: - Buffer Preparation
    
    private func prepareAlignedBuffer(_ audio: [Float], using buffer: UnsafeMutableRawPointer) throws -> [Float] {
        let alignedCount = (audio.count + 63) & ~63  // 64-byte alignment
        
        // Use vDSP for fast copy with alignment
        audio.withUnsafeBufferPointer { src in
            let dst = buffer.assumingMemoryBound(to: Float.self)
            vDSP_mmov(src.baseAddress!, dst, vDSP_Length(audio.count), 1, 1, 1)
            
            // Zero-pad to alignment
            if alignedCount > audio.count {
                let padStart = dst.advanced(by: audio.count)
                memset(padStart, 0, (alignedCount - audio.count) * MemoryLayout<Float>.size)
            }
        }
        
        return Array(UnsafeBufferPointer(
            start: buffer.assumingMemoryBound(to: Float.self),
            count: alignedCount
        ))
    }
    
    // MARK: - Cache Key
    
    private func computeCacheKey(for audio: [Float]) -> String {
        // Fast hash of first/last 100 samples
        let prefix = audio.prefix(100)
        let suffix = audio.suffix(100)
        
        var hash = 0
        for (i, sample) in prefix.enumerated() {
            hash ^= Int(sample * 1000) << (i % 32)
        }
        for (i, sample) in suffix.enumerated() {
            hash ^= Int(sample * 1000) << ((i + 100) % 32)
        }
        
        return "\(audio.count)_\(hash)"
    }
    
    // MARK: - Performance Metrics
    
    func getLastInferenceTime() -> TimeInterval {
        return lastInferenceTime
    }
    
    func clearCache() {
        kvCache.removeAll()
        logger.info("üßπ Cleared KV-cache")
    }
}

// MARK: - Buffer Pool
private class BufferPool {
    private var availableBuffers: [UnsafeMutableRawPointer] = []
    private let bufferSize = 192000 * MemoryLayout<Float>.size  // 3s @ 16kHz
    private let maxBuffers = 5
    private let lock = NSLock()
    
    init() {
        // Pre-allocate buffers
        for _ in 0..<maxBuffers {
            let buffer = UnsafeMutableRawPointer.allocate(
                byteCount: bufferSize,
                alignment: 64  // 64-byte aligned
            )
            availableBuffers.append(buffer)
        }
    }
    
    func acquire() -> UnsafeMutableRawPointer {
        lock.lock()
        defer { lock.unlock() }
        
        if let buffer = availableBuffers.popLast() {
            return buffer
        } else {
            // Fallback: allocate new buffer
            return UnsafeMutableRawPointer.allocate(
                byteCount: bufferSize,
                alignment: 64
            )
        }
    }
    
    func release(_ buffer: UnsafeMutableRawPointer) {
        lock.lock()
        defer { lock.unlock() }
        
        if availableBuffers.count < maxBuffers {
            // Clear buffer and return to pool
            memset(buffer, 0, bufferSize)
            availableBuffers.append(buffer)
        } else {
            // Pool is full, deallocate
            buffer.deallocate()
        }
    }
    
    deinit {
        for buffer in availableBuffers {
            buffer.deallocate()
        }
    }
}

// MARK: - Errors
enum WhisperError: LocalizedError {
    case modelNotLoaded
    case transcriptionFailed
    case bufferPreparationFailed
    
    var errorDescription: String? {
        switch self {
        case .modelNotLoaded:
            return "WhisperKit model not loaded"
        case .transcriptionFailed:
            return "Transcription failed"
        case .bufferPreparationFailed:
            return "Failed to prepare aligned buffer"
        }
    }
}

// MARK: - Extensions for MLMultiArray Shape
extension MLMultiArray.Shape {
    static func enumerated(batch: Int, channels: Int, height: Int, sequence: Int) -> [NSNumber] {
        return [batch, channels, height, sequence].map { NSNumber(value: $0) }
    }
}

// MARK: - Extensions for Model Parameters
private extension String {
    static let palettizationBits = "palettizationBits"
    static let groupedChannelPalettization = "groupedChannelPalettization"
    static let quantizationType = "quantizationType"
    static let optimizeForNeuralEngine = "optimizeForNeuralEngine"
}